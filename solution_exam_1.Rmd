---
title: "STA 522 Exam 1 Solutions"
author: ""
date: ""
output: pdf_document
---

\newcommand{\rs}{X_1,X_2,\dots,X_n}
\newcommand{\on}{\operatorname}
\newcommand{\enter}{\vspace{0.1in}}
\newcommand{\ds}{\displaystyle}
\renewcommand{\bar}{\overline}
\newcommand{\N}{\text{N}}
\renewcommand{\epsilon}{\varepsilon}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Ss}{\mathcal{S}}
\newcommand{\E}{\on{E}}
\newcommand{\var}{\on{Var}}
<!-- \renewcommand{\vec}[1]{{\underaccent{\tilde}{#1}}} -->
\renewcommand{\vec}{\underline}
\newcommand{\asim}{\stackrel{a}{\sim}}
\newcommand{\points}[1]{\hfill \textbf{(#1 pts)}}




# Problem 1

**Part (a):** Since $\rs$ are iid Uniform(0, 1), the cdf of each $X_i$ is given by:
$$
F(x) = \begin{cases}
0 & \text{if } x \leq 0 \\
{x} & \text{if } 0 < x < 1 \\
1 & \text{if } x \geq 1
\end{cases}
$$
Therefore, 
$$
\begin{aligned}
P\left(X_{(1)} < 0.25\right) 
&= 1- \left(X_{(1)} \geq 0.25\right) \\ 
&= 1 - P\left(X_i \geq 0.25 \text{ for all } i \right) \\
&= 1 - \{1-F(0.25)\}^n && \text{(iid)} \\
&= 1 - (1 - 0.25)^n = \boxed{1 - (0.75)^n}
\end{aligned}
$$
and
$$
\begin{aligned}
P\left(X_{(n)} < 0.25\right) 
&= P\left(X_i < 0.25 \text{ for all } i \right) \\
&= \{F(0.25)\}^n && \text{(iid)} \\
&= \boxed{(0.25)^n}
\end{aligned}
$$

Because $X_{(1)} \geq X_{(n)}$, therefore $X_{(n)} < 0.25$ *implies* $X_{(1)} < 0.25$, so that $P(X_{(n)} < 0.25) \leq P(X_{(1)} < 0.25)$.

**Part (b):** Yes, it does. We'll first show that $X_{(n)} \xrightarrow{P} 1$. This is similar to the solution for Problem 1(b) in the sample exam, with the difference being that here we have a Uniform$(0, 1)$ population instead of a Uniform$(-1, 1)$ population.  

Fix $\epsilon > 0$ small. We have
$$
\begin{aligned}
P(|X_{(n)} - 1| \geq \epsilon) &= P(X_{(n)} - 1 \geq \epsilon) + P(X_{(n)} - 1 < -\epsilon) \\
&=  P(X_{(n)} \geq 1 + \epsilon) + P(X_{(n)}  < 1 -\epsilon) \\
&= 0 + P(X_i < 1 -\epsilon, \text{ all } i) \\
&= \left\{P(X_1 < 1-\epsilon)\right\}^n && \text{(iid)} \\
&= \begin{cases}
\left(1-\epsilon\right)^n & \text{ if } \epsilon < 1 \\
0 & \text{if } \epsilon \geq 1
\end{cases} \\
&\to  0 \qquad \text{as } {n \to \infty}
\end{aligned}
$$
which means $X_{(n)} \xrightarrow{P} 1$. 


Now apply the continuous mapping result: if $X_n \xrightarrow{P} X$ then $h(X_n) \xrightarrow{P} X$ for any continuous funciton $h$. Because here $X_{(n)} \xrightarrow{P} 1$ and $h(x) = x/2$ is a continuous function, therefore, $X_{(n)}/2 \xrightarrow{P} 1/2$.


# Problem 2

<!-- **Part (a):**  -->
Fix $\epsilon > 0$. Then
$$
\begin{aligned}
P(|X_n - 0| \geq \epsilon) 
= P\left(X_n^2 \geq \epsilon^2\right) 
\leq P\left(\frac{X_n^2}{1 + X_n^2} \geq \frac{\epsilon^2}{1+\epsilon^2}\right)  
\stackrel{(*)}{\leq}  \frac{ \E\left[\frac{X_n^2}{1 + X_n^2}\right]}{ \frac{\epsilon^2}{1+\epsilon^2}}
\to  0 \quad \text{as } {n \to \infty}
\end{aligned}
$$
where $(\star)$ is due to Chebyshev's inequality $\left(\frac{X_n^2}{1 + X_n^2} \text{ is non-negative} \right)$. This implies that $X_n \xrightarrow{P} 0$.



<!-- **Part (b):** Fix $\epsilon > 0$. Consider an arbitrary $\delta > 0$. As suggested in the hint, there exists $k = k_\delta > 0$ such that $P(|Y| > k) < \delta$. Therefore, -->
<!-- $$ -->
<!-- \begin{aligned} -->
<!-- P(|X_n Y - XY| \geq \epsilon)  -->
<!-- &= P(|X_n - X| \ |Y| \geq \epsilon) \\  -->
<!-- &= P(|X_n - X| \ |Y| \geq \epsilon, |Y| > k) +  P(|X_n - X| \ |Y| \geq \epsilon, |Y| \leq k) \\ -->
<!-- &\leq  P(|Y| > k) + P(|X_n - X| \ k \geq \epsilon) \\ -->
<!-- &< \delta + P(|X_n - X| \geq \epsilon/k) && \left(\text{since } P(|Y| > k) < \delta\right) \\ -->
<!-- & \xrightarrow{n \ \to \ \infty}  \delta + 0  = \delta && (X_n \xrightarrow{P} X) \\ -->
<!-- & \xrightarrow{\delta \ \to \ 0} 0. && (\delta \text{ is arbitrary}) -->
<!-- \end{aligned} -->
<!-- $$ -->
<!-- This proves that $X_n Y \xrightarrow{P} XY$. -->



# Problem 3

<!-- **Part (a):** \underline{Sufficiency:} There is only one random observation $X$  which constitutes the entire sample. Therefore, $X$ is trivially sufficient. -->


<!-- \underline{Completeness:} consider -->
<!-- $$ -->
<!-- \begin{aligned} -->
<!-- \E_\theta(X) &= (-1) \ P_\theta(X = -1) + (0) \ P_\theta(X = 0) + (1) \ P_\theta(X = 1) \\ -->
<!-- &= (-1) \left(\frac{\theta}{2}\right)^{1} (1-\theta)^{1-1} + (1) \left(\frac{\theta}{2}\right)^{1} (1-\theta)^{1-1} = 0 \quad  \text{ for all } \theta \\ -->
<!-- \end{aligned} -->
<!-- $$ -->
<!-- Thus the function $g(X) = X$ is such that $E_\theta[g(X)] = 0$ for all $\theta$, but $P_\theta(g(X) = 0) = P_\theta(X = 0) = \left(\frac{\theta}{2}\right)^{0} (1-\theta)^{1-0} = 1-\theta \neq 1$ for any $0 < \theta < 1$. Hence $X$ is NOT complete. -->

<!-- \vspace{0.2in} -->

**Part (a):**  \underline{Sufficiency:} The pmf of $X$ is
$$
f(x \mid \theta ) = \left(\frac{\theta}{2}\right)^{|x|} (1-\theta)^{1-|x|} = \underbrace{\theta^{|x|} (1-\theta)^{1-|x|}}_{=g(T(x) \mid \theta)} \ \underbrace{2^{- |x| }}_{=h(x)}
$$
where $T(x) = |x|$. Therefore, by the Factorization theorem, $|X|$ is sufficient for $\theta$.



**Part (b):**  \underline{Completeness:} As suggested in the hint, we first find the pmf of $Y = |X|$. We note that the support of $Y$ is $\{0, 1\}$. Clearly, $P(Y = 0) = P(X = 0) = \left(\frac{\theta}{2}\right)^{0} (1-\theta)^{1-0} = 1-\theta$, and 
$$
\begin{aligned}
P(Y = 1) &= P(X = 1) + P(X = -1) \\
&= \left(\frac{\theta}{2}\right)^{1} (1-\theta)^{1-1} + \left(\frac{\theta}{2}\right)^{1} (1-\theta)^{1-1} = \theta
\end{aligned}
$$
Thus, 
$$
P(Y = y) = \begin{cases}
\theta & y=1 \\
1-\theta & y=0
\end{cases}
$$
for $0 < \theta < 1$, which means $Y \sim \on{Bernoulli}(\theta)$ for $0 < \theta < 1$. Therefore, by the completeness of Binomial family (proved in class) it follows that $Y = |X|$ is complete.
 

# Problem 4

Let $\vec x = (x_1, \dots, x_n)$ and $\vec y = (y_1, \dots, y_n)$ be two sample points from the density $f(x \mid \theta)$.

<!-- **Part(a):** We have -->
<!-- $$ -->
<!-- \begin{aligned} -->
<!-- \frac{f(\vec x \mid \theta)}{f(\vec y \mid \theta)} -->
<!-- &= \frac{\prod_{i=1}^n \frac{\gamma}{\theta} \ x_i^{\gamma-1} e^{-x_i^\gamma/\theta}}{\prod_{i=1}^n \frac{\gamma}{\theta} \ y_i^{\gamma-1} e^{-y_i^\gamma/\theta}} = \frac{\prod_{i=1}^n x_i^{\gamma-1} }{\prod_{i=1}^n y_i^{\gamma-1}} \ \exp\left[ -\frac{1}{\theta} \left(\sum_{i=1}^n x_i^{\gamma} - \sum_{i=1}^n y_i^{\gamma}\right)\right] -->
<!-- \end{aligned} -->
<!-- $$ -->
<!-- This is constant as a function of $\theta$ \underline{if and only if} $\sum_{i=1}^n x_i^{\gamma} = \sum_{i=1}^n y_i^{\gamma}$. Therefore, $\sum_{i=1}^n X_i^{\gamma}$ is minimal sufficient for $\theta$. -->

**Part (a):** \underline{Sufficiency:} We'll use the Factorization theorem on the joint density:
$$
\begin{aligned}
f(\vec x \mid \lambda) &= \prod_{i=1}^n \frac{1}{\lambda} \exp\left[-\frac{1}{\lambda} (x_i - \mu)\right] I (x_i > \mu) \\
&= \exp\left[ -\frac{1}{\lambda} \sum_{i=1}^n x_i \right] I(x_{(1)} > \mu) \\ 
&= g(T_1(\vec x), T_2(\vec x) \mid \lambda, \mu) \ h(\vec x)
\end{aligned}
$$
where $T_1(\vec x) = \sum_{i=1}^n x_i$, $T_2(\vec x) = x_{(1)}$, $g(t_1, t_2 \mid \lambda, \mu) = \exp(-t_1/\lambda) \ I(t_2 > \mu)$. Therefore, by the Factorization theorem, $(\sum_{i=1}^n X_i, X_{(1)})$ is jointly sufficient for $(\lambda, \mu)$. 

**Part (b):** \underline{Minimal Sufficiency:} We have 
$$
\begin{aligned}
\frac{f(\vec x \mid \mu, \lambda)}{f(\vec y \mid \mu, \lambda)}
&= \frac{\prod_{i=1}^n \frac{1}{\lambda} \exp\left[-\frac{1}{\lambda} (x_i - \mu)\right] I (x_i > \mu)}{\prod_{i=1}^n \frac{1}{\lambda} \exp\left[-\frac{1}{\lambda} (y_i - \mu)\right] I (y_i > \mu)} \\ 
&= \exp\left[ -\frac{1}{\lambda} \left(\sum_{i=1}^n x_i - \sum_{i=1}^n y_i\right)\right] \frac{\prod_{i=1}^n I(x_i > \mu) }{\prod_{i=1}^n I(y_i > \mu)} \\ 
&= \exp\left[ -\frac{1}{\lambda} \left(\sum_{i=1}^n x_i - \sum_{i=1}^n y_i\right)\right] \frac{I(x_{(1)} > \mu) }{ I(y_{(1)} > \mu)}
\end{aligned}
$$
This is constant as a function of $(\mu, \lambda)$ \underline{if and only if}
$x_{(1)} = y_{(1)}$ and $\sum_{i=1}^n x_i = \sum_{i=1}^n y_i$. Therefore, $(X_{(1)},  \sum_{i=1}^n X_i)$ is minimal sufficient for $(\mu, \lambda)$.

<!-- **Part (c):** We have -->
<!-- $$ -->
<!-- \begin{aligned} -->
<!-- \frac{f(\vec x \mid \theta)}{f(\vec y \mid \theta)} -->
<!-- &= \frac{\prod_{i=1}^n \frac{1}{\theta} I (\theta < x_i < 2 \theta)}{\prod_{i=1}^n \frac{1}{\theta} I (\theta < y_i < 2 \theta)} -->
<!-- = \frac{I (\theta < x_{(1)} < x_{(n)} < 2 \theta)}{I (\theta < x_{(1)} < x_{(n)} < 2 \theta)} = \frac{I (x_{(n)}/2 < \theta < x_{(1)})}{I (y_{(n)}/2 < \theta < y_{(1)})} -->
<!-- \end{aligned} -->
<!-- $$ -->
<!-- This is constant as a function of $\theta$ \underline{if and only if} $x_{(1)} = y_{(1)}$ and $x_{(n)} = y_{(n)}$. Therefore, $(X_{(1)}, X_{(n)})$ is minimal sufficient for $\theta$. -->



# Problem 5

**Part (a):** Let us denote by $f(x - \mu)$ the common location family density of $\rs$. Then there exist iid observations $Z_1, \dots, Z_n$ from the density $f(x)$ (the standard density of the family) such that $Z_i = X_i - \mu$, i.e., $X_i = Z_i + \mu$.

Note that the sample median is: 
$$
\begin{aligned}
M(\rs) &= 
\begin{cases}
X_{\left(\frac{n+1}{2}\right)} & n \text{ is odd} \\
\frac{X_{\left(\frac{n}{2}\right)} + X_{\left(\frac{n}{2} + 1\right)}}{2} & n \text{ is even}
\end{cases} \\
&= \begin{cases}
\mu + Z_{\left(\frac{n+1}{2}\right)} & n \text{ is odd} \\
\mu + \frac{Z_{\left(\frac{n}{2}\right)} + Z_{\left(\frac{n}{2} + 1\right)}}{2} & n \text{ is even}
\end{cases} \\
&= \mu + M(Z_1, \dots, Z_n)
\end{aligned}
$$
Again,
$$
\bar X = \frac{1}{n} \sum_{i=1}^n X_i = \frac{1}{n} \sum_{i=1}^n (\mu + Z_i) = \mu + \bar{Z}
$$
Hence
$$
M(\rs) - \bar X = M(Z_1, \dots, Z_n) - \bar Z
$$
where the RHS contains random variables whose distribution does not depend on the parameter $\mu$. Hence $M - \bar X$ is an ancillary statistic.

<!-- **Part (b):** Note at the outset that conditional on $N = n$, $X \mid N = n \sim \on{Binomial}(n, \theta)$.  -->



<!-- \underline{Minimal Sufficiency:} Let $(x, n)$ and $(y, m)$ be two sample points from the joint pmf of $(X, N)$.  Then -->
<!-- $$ -->
<!-- \begin{aligned} -->
<!-- \frac{P_\theta(X = x, N = n)}{P_\theta(X = y, N = m)} -->
<!-- &= \frac{P_\theta(X = x \mid N = n) \ P(N = n)}{P_\theta(X = y \mid N = m) \ P(N = m)} =  -->
<!-- \frac{\binom{n}{x} \theta^x (1-\theta)^{n-x} \ p_n}{\binom{m}{y} \theta^y (1-\theta)^{m-y} \ p_m} -->
<!-- = \frac{\binom{n}{x} p_n}{\binom{m}{y} p_m} \ \theta^{x-y} (1-\theta)^{(n-x)-(m-y)} -->
<!-- \end{aligned} -->
<!-- $$ -->
<!-- This is constant as a function in $\theta$ \underline{if and only if} $x = y$ and $n-x = m - y$, i.e., $x = y$ and $n = m$. Hence $(X, N)$ is minimal sufficient for $\theta$. -->

<!-- \underline{N is ancillary:} Clearly, the pmf of $N$ is $P(N = n) = p_n$ which is free of $\theta$. Hence $N$ is ancillary.  -->



<!-- # Problem 6 -->

<!-- **Part (a):** The derivation is essentially similar to that for a binomial distribution, which has been derived in class. -->

<!-- \underline{Sufficiency:} The joint pmf of $X_1, \dots, X_n$ is  -->
<!-- $$ -->
<!-- f(\vec x \mid \lambda) = \prod_{i=1}^n e^{-\lambda} \frac{\lambda^{x_i}}{x_i!} = \underbrace{e^{-n\lambda} \lambda^{\sum_{i=1}x_i}}_{=g(T(\vec x \mid \lambda))} \underbrace{ \prod_{i=1}^n \frac{1}{x_i!}}_{=h(\vec x)} -->
<!-- $$ -->
<!-- where $T(\vec x) = \sum_{i=1}^n x_i$. Therefore, by the factorization theorem, $\sum_{i=1}^n X_i$ is sufficient for $\lambda$. -->


<!-- \underline{Completeness:}  First obtain the pmf of $Y = \sum_{i=1}^n X_i$. The mgf of each $X_i$ is $M(t) = e^{\lambda (e^t-1)}$. The mgf of $Y = \sum_{i=1}^n X_i$ is -->
<!-- $$ -->
<!-- M_Y(t) = \E\left(e^{tY}\right) = \E\left(e^{t\sum_{i=1}^n X_i}\right) \overset{\on{iid}}{=} \left[\E(e^{tX_1})\right]^n = M(t)^n = e^{n\lambda (e^t  -1)} -->
<!-- $$ -->
<!-- which is the mgf of $\on{Poisson}(n\lambda)$. Therefore, by the uniqueness of mgf, $\sum_{i=1}^n X_i \sim \on{Poisson}(n\lambda)$. -->


<!-- Consider any function  $g(y)$ of $y = \sum_{i=1}^n$. Then -->
<!-- $$ -->
<!-- \E_\lambda(g(Y)) = \sum_{y=0}^\infty g(y) \ P_\lambda(Y = y) = \sum_{y=0}^\infty g(y)\  \ \underbrace{e^{-n\lambda} \frac{(n\lambda)^y}{y!}}_{>0, \text{ for all } \lambda>0} -->
<!-- $$ -->
<!-- Therefore $E_\lambda (g(Y)) = 0$ for all $\lambda > 0$ means $g(y) = 0$ for $y = 0, 1, \dots$, i.e., $P_\lambda(g(Y) = 0) = 1$. This means that $Y$ is complete. -->


**Part (b):** As suggested in the hint consider a sequence $Y_n$ where $Y_n = X$, and $W = 1-X$, where $X \sim \on{Binomial}(1, 0.5)$. Then $X$ and $W = 1-X$ have the same distribution. Then $Y_n \xrightarrow{d} X$ (trivially; all have the same distribution) which means $Y_n \xrightarrow{d} W$ as $X$ and $W$ have the same distribution.

However, for any $0 < \epsilon < 1$,
$$
\begin{aligned}
P(|Y_n - W| \geq \epsilon) = P(|X - 1 + X| \geq \epsilon) 
&= P(|2X - 1| \geq \epsilon) = P(2X \geq 1 + \epsilon) + P(2X \leq 1 - \epsilon) \\
&= P\left(X \geq \frac{1 + \epsilon}{2}\right) + P\left(X \leq \frac{1 - \epsilon}{2}\right) \\
&= P(X = 1) + P(X = 0) = 1 \not \rightarrow 0
\end{aligned}
$$
as $n \to \infty$. Hence $Y_n \not \xrightarrow{P} W$.




