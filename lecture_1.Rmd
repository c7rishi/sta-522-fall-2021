---
title: |
  | STA 522, Spring 2021  
  | Introduction to Theoretical Statistics II
author: | 
  | Lecture 1
  | 
  | Department of Biostatistics
  | University at Buffalo 
date: "01 February, 2021"
output: 
  beamer_presentation:
    toc: false
fontsize: 11pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{=tex}
\newcommand{\rs}{X_1,X_2,\dots,X_n}
\newcommand{\on}{\operatorname}
\newcommand{\enter}{\vspace{0.1in}}
\newcommand{\ds}{\displaystyle}
\renewcommand{\bar}{\overline}
\newcommand{\N}{\text{N}}
\renewcommand{\epsilon}{\varepsilon}
\newcommand{\R}{\mathbb{R}}
```
# Agenda

-   Review random samples

-   Order Statistics

-   Convergence Concepts

----

# Review: Random Samples

**Definition:** The random variables $X_1,X_2,\dots,X_n$ are called a **random sample** of size $n$ from the population $f(x)$ if $X_1,X_2,\dots,X_n$ are mutually independent random variables and the marginal pdf or pmf of each $X_i$ is the same function $f(x)$.

\vspace{0.1in}

**Notation:** $X_1,X_2,\dots,X_n\sim \operatorname{iid} f$. Joint pdf/pmf: $f_{X_1,X_2,\dots,X_n}(x_1, \dots, x_n) = f(x_1, \dots, x_n) := \prod_{i=1}^n f(x_i)$

\vspace{0.1in}

If $f$ is a member of a parametric family with parameter(s) $\theta$, then we may write $f(x_1, x_2, \dots, x_n \mid \theta)=\prod_{i=1}^n f(x_i \mid \theta)$

\vspace{0.1in}

Example: $X_1,X_2,\dots,X_n\sim \operatorname{iid} \text{N}(\mu, \sigma^2)$ with $f(x \mid \mu, \sigma^2) = \frac{1}{\sqrt{2\pi} \sigma} \exp \left( -\frac{1}{2\sigma^2} (x - \mu)^2 \right)$

----

# Review: Statistics and Sampling Distributions

**Definition:** Let $X_1,X_2,\dots,X_n$ be a random sample of size $n$ from a population and let $T(x_1,x_2,\ldots,x_n)$ be a function (real-valued or vector-valued) whose domain includes the sample space of $(X_1,X_2,\dots,X_n)$. The random variable (or vector) $Y=T(X_1,X_2,\dots,X_n)$ is called a **statistic**. The probability distribution of a statistic is called its **sampling distribution**.

\vspace{0.1in}

**Note:** A statistic cannot contain a parameter.

\vspace{0.1in}

Examples:

(i) sample mean $\overline X = \frac1n \sum_{i=1}^n X_i$,

(ii) sample variance $S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \overline X)^2 = \frac{1}{n-1} \sum_{i=1}^n X_i^2 - \frac{n}{n-1} \overline X^2$

(iii) sample standard deviation $S = \sqrt{S^2}$.

(iv) sample minimum, sample maximum, sample quantiles.

----

**Result (Lemma 5.2.5):** Let $X_1,X_2,\dots,X_n$ be a random sample from a population, and let $g(x)$ be a function such that $\operatorname{E}{(g(X_1))}$ and $\operatorname{Var}{(g(X_1))}$ exist. Then $$\operatorname{E}{\left(\sum_{i=1}^n{g(X_i)}\right)}=n\operatorname{E}{(g(X_1))}$$and $$\operatorname{Var}{\left(\sum_{i=1}^n{g(X_i)}\right)}=n\operatorname{Var}{(g(X_1))}.$$

**Result (Theorem 5.2.6):** Let $X_1,X_2,\dots,X_n$ be a random sample from a population with mean $\mu$ and variance $\sigma^2<\infty$. Then

(a) $\operatorname{E}{(\overline{X})}=\mu$

(b) $\displaystyle{\operatorname{Var}{(\overline{X})}=\frac{\sigma^2}{n}}$; and

(c) $\operatorname{E}{(S^2)}=\sigma^2$.

----

How to determine the sampling distribution of $\overline X$?

(i) **Using transformations.** Let ${Y=\sum_{i=1}^n{X_i}}$, so that $\overline{X}=\frac{1}{n}Y$. If $f(x)$ is the pdf of $Y$, then the pdf of $\overline{X}$ is $f_{\overline{X}}(x)=nf(nx)$.

(ii) **Using mgf (Theorem** $5.2.7$**)**. $M_{\overline{X}}(t)=M_Y\left(\frac{t}{n}\right)=\left[M_X\left(\frac{t}{n}\right)\right]^n$ where $M_X(t)$ is the mgf of the underlying population. Then identify the distribution of $\overline X$.

\vspace{0.1in}

**Theorem 5.3.1.** Let $X_1,X_2,\dots,X_n\sim \operatorname{iid} \text{N}(\mu, \sigma^2)$. Then

a\. $\overline X$ and $S^2$ are independent random variables.

b\. $\overline X \sim \text{N}(\mu, \sigma^2/n)$.

c\. $(n-1)S^2/\sigma^2 \sim \chi^2_{n-1}$.

----

# Order Statistics

**Definition:** The order statistics of a random sample $X_1,X_2,\dots,X_n$ are the sample values placed in ascending order. They are denoted by $X_{(1)},X_{(2)},\ldots,X_{(n)}$ and satisfy $X_{(1)}\leq X_{(2)}\leq\cdots\leq X_{(n)}$.

\vspace{0.1in}

\vspace{0.1in}

**Examples:**

(a) **sample minimum:** $X_{(1)}$ and **sample maximum:** $X_{(n)}$ are called the extreme order statistics.
(b) **sample range:** $R=X_{(n)}-X_{(1)}$.
(c) **sample median:** $M$ where$$M=\begin{cases}X_{((n+1)/2)}&\text{if }n\text{ is odd;}\\[1em]
    \displaystyle{\frac{X_{(n/2)}+X_{(n/2+1)}}{2}}&\text{if }n\text{ is even.}
    \end{cases}$$

----

# Sampling Distributions of Extreme Order Statistics from a Continuous Population

Suppose $X_1,X_2,\dots,X_n$ is a random sample from a population with continuous cdf $F$ and pdf $f$. Then

1.  $\{ X_{(n)} \leq x \} = \{\text{all } X_i \leq x\} = \{X_1 \leq x, \dots, X_n \leq x \}$. So $$ 
    \begin{aligned}
    F_{X_{(n)}}(x) 
    &= P(X_{(n)} \leq x) \\
    &= P(X_1 \leq x, \dots, X_n \leq x) \\ 
    &= P(X_1 \leq x) \dots P(X_n \leq x) \\
    &= F(x) \dots F(x) = [F(x)]^n
    \end{aligned}
    $$ Differentiating, $f_{X_{(n)}}(x) = n \ f(x) \left[F(x)\right]^{n-1}$.

\vspace{0.1in}

2.  $\{ X_{(1)} \geq x \} = \{\text{all } X_i \geq x\} = \{X_1 \geq x, \dots, X_n \geq x \}$. Implies $F_{X_{(1)}}(x) = 1 - [1 - F(x)]^n$ & $f_{X_{(1)}} (x) = n\ f(x) \left[1-F(x)\right]^{n-1}$.

----

**Example:** $X_1,X_2,\dots,X_n\sim\operatorname{iid~Uniform}(0,\theta)$. Find the pdf and the expected value of $X_{(n)}$. $$
 \text{Here } f(x \mid \theta) = \frac{1}{\theta}\ I(0 \leq x \leq \theta)
\text{ and }
F(x \mid \theta) = 
\begin{cases} 
0, & x < 0 \\ 
\frac{x}{\theta}, & 0 \leq x \leq \theta \\ 
1, & x > 1  
\end{cases}
$$ so that $$
\begin{aligned}
f_{X_{(n)}}(x \mid \theta) &= n \ f(x \mid \theta) \left[F(x  \mid \theta)\right]^{n-1} \\
&= n \left(\frac{1}{\theta} \right) \left(\frac{x}{\theta}\right)^{n-1} I(0 \leq x \leq \theta) \\
&= \frac{n \ x^{n-1}}{\theta^n} \ I(0 \leq x \leq \theta)
\end{aligned}
$$ Find expected value $\operatorname{E} \left[ X_{(n)} \right] = \operatorname{E} \left[ X_{(n)} \mid \theta \right]$ using integration: $$
\begin{aligned}
\operatorname{E} \left[ X_{(n)} \right] &= 
\int_{-\infty}^{\infty} x \ f_{X_{(n)}}(x \mid \theta) \ dx
= \frac{n}{\theta^n} \int_{0}^{\theta} x^n  \ dx 
= \frac{n}{n+1}\ \theta
\end{aligned}
$$

----

# Distribution of a general order statistic from a continuous population

**Theorem 5.4.4.** Let $X_{(1)},X_{(2)},\ldots,X_{(n)}$ denote the order statistics of a random sample $X_1,X_2,\dots,X_n$ from a continuous population with cdf $F(x)$ and pdf $f(x)$. The pdf of $X_{(j)}$ is $$
f_{X_{(j)}}(x) = \frac{n!}{(j-1)!(n-j)!} \ f(x) \ \left[F(x)\right]^{j-1} \left[1-F(x)\right]^{n-j}.
$$

**Partial Proof.** Call $\{X_i \leq x\}$ a "success", $\{X_i > x\}$ a "failure". Define $Z_i = I(X_i \leq x)$ and $Y = \sum_{i=1}^n Z_i$. Note that $Z_i \sim \operatorname{iid~Bernoulli} (F(x))$ $\implies$ $Y \sim \operatorname{Binomial}(n, F(x))$. Note that,$$
F_{X_{(j)}}(x) = P(X_{(j)} \leq x) = P(Y \geq j) = \sum_{k=j}^n \binom{n}{k}\ [F(x)]^k \ [1 - F(x)]^{n-k} 
$$

The pdf is obtained using differentiation.

----

# Distribution of a general order statistic from a discrete population

**Theorem 5.4.3.** Let $X_1,X_2,\dots,X_n$ be a random sample from a discrete distribution with pmf $f(x_i) = p_i$, where $x_1 < x_2 < \dots$ are the possible values of $X$ in ascending order. Let $X_{(1)}, X_{(2)}, \dots, X_{(n)}$ denote the order statistics from the sample. Define $$
\begin{aligned}
P_0 & = 0\\
P_i & = p_1+p_2+\cdots+p_i\qquad\text{for }i\geq1\\
\end{aligned} 
$$ Then $$
\begin{aligned}
P(X_{(j)}\leq x_i) & = \sum_{k=j}^n{\binom{n}{k}P_i^k(1-P_i)^{n-k}}\\
P(X_{(j)}=x_i) & = \sum_{k=j}^n{\binom{n}{ k}\left[P_i^k(1-P_i)^{n-k}-P_{i-1}^k(1-P_{i-1})^{n-k}\right]}.
\end{aligned}
$$

**Proof:** cdf is similar to the continuous case. The pmf is obtained from the cdf through $P(X_{(j)} = x_i) = P(X_{(j)} \leq x_i) - P(X_{(j)} \leq x_{i-1})$.

----

**Example:** $X_1,X_2,\dots,X_n\sim\operatorname{iid~Uniform}(0,1)$. Find the distribution of the $j^{\text{th}}$ order statistic, along with its mean and variance.

Here $f(x) = I(0 < x < 1)$ and $F(x) = x$ for $0 < x < 1$. Therefore

$$
\begin{aligned}
f_{X_{(j)}}(x) &= \frac{n!}{(j-1)!(n-j)!} \ f(x) \ \left[F(x)\right]^{j-1} \left[1-F(x)\right]^{n-j} \\
&= \frac{n!}{(j-1)!(n-j)!} \ x^{j-1} \ (1-x)^{n-j} \ I(0 < x < 1) \\
&= \frac{\Gamma(n+1)}{\Gamma(j)\Gamma(n-j+1)} \ x^{j-1} \ (1-x)^{n-j} I(0 < x < 1)
\end{aligned}
$$ This shows that $X_{(j)} \sim \operatorname{Beta}(j, n-j+1)$. From this, we can deduce that $$
\operatorname{E}\left[ X_{(j)} \right] = \frac{j}{n+1}
$$ and $$
\operatorname{Var}\left[ X_{(j)} \right] = \frac{j(n-j+1)}{(n+1)^2(n+2)}.
$$

----

# Joint Distribution of Order Statistics

**Theorem 5.4.6.** Let $X_{(1)},X_{(2)},\ldots,X_{(n)}$ denote the order statistics of a random sample $X_1,X_2,\dots,X_n$ from a continuous population with cdf $F(x)$ and pdf $f(x)$. The joint pdf of $X_{(i)}$ and $X_{(j)}$, $1\leq i<j\leq n$, is $$ 
f_{X_{(i)},X_{(j)}}(u,v) = c\ f(u)\ f(v) \left[F(u)\right]^{i-1} \left[F(v) - F(u)\right]^{j-1-i} \left[1-F(v)\right]^{n-j}
$$

for $-\infty<u<v<\infty$, where $c=\frac{n!}{(i-1)!(j-1-i)!(n-j)!}.$

\vspace{0.1in}

Joint distribution pdf of all the order statistics from a continuous population: $$
f_{X_{(1)}, \dots, X_{(n)}} (x_1, \dots, x_n) = 
\begin{cases}
n! f(x_1) \dots f(x_n), & -\infty < x_1 < \dots < x_n < \infty \\
0, & \text{otherwise}
\end{cases}
$$

----

**Example:** Let $X_1,X_2,\dots,X_n\sim\operatorname{iid~uniform}(0,a)$, $X_{(1)},X_{(2)},\ldots,X_{(n)}$ denote the order statistics. Find the joint pdf of the sample range $R = X_{(n)} - X_{(1)}$ and the mid-range $V = \frac{X_{(1)}+X_{(n)}}{2}$. Hence find the marginal pdf of $R$.

\vspace{0.1in}

First obtain the joint pdf of $X_{(1)}$ and $X_{(n)}$: $$
\begin{aligned}
f_{X_{(1)}, X_{(n)}} (x_1, x_n) 
&= \frac{n(n-1)}{a^2} \left(\frac{x_n}{a} - \frac{x_1}{a}\right)^{n-2} \ I(0 < x_1 < x_n < a) \\
&= \frac{n(n-1) (x_n - x_1)^{n-2}}{a^n} \ I(0 < x_1 < x_n < a)
\end{aligned}
$$

Solve for $X_{(1)}, X_{(n)}$ to obtain $X_{(1)} = V - R/2$ and $X_{(n)} = V + R/2$. Jacobian of this transformation is -1.

----

Support of $(R, V)$: $$
\begin{aligned}
& 0 < x_1 < x_n < a \\
\implies & 0 < v - r/2 < v + r/2 < a \\  
\implies & 0 < r < a,\ r/2 < v < a - r/2
\end{aligned}
$$ The joint pdf of $(R, V)$ is $$
f_{R, V}(r, v) = \frac{n(n-1)\ r^{n-2}}{a^n}; \ \ 0 < r < a,\ r/2 < v < a - r/2
$$ The marginal pdf of $R$ is $$
\begin{aligned}
f_R(r) = \int_{r/2}^{a - r/2}  \frac{n(n-1)\ r^{n-2}}{a^n}\ dv = \frac{n(n-1)\ r^{n-2} \ (a-r)}{a^n}; \ 0 < r < a
\end{aligned}
$$

It is easy to see that $\frac{R}{a} \sim \operatorname{Beta}(n-1, 2)$ distribution.

\vspace{0.1in}

**HW:** find the marginal pdf of $V$.

----

# Convergence Concepts

What happens to sample statistics, particularly $\overline X = {\overline X}_n$, when the sample size $n \to \infty$?

\vspace{0.1in}

For a real sequence $(a_n)_{n=1}^\infty$ defining convergence is straightforward: $(a_n)_{n=1}^\infty$ is said to converge to a point $a$ if $\lim_{n \to \infty} |a_n - a| = 0$.

\vspace{0.1in}

How to define convergence of random variables?

-   convergence in probability
-   convergence in almost sure sense
-   convergence in distribution (or law)
-   convergence in mean (or norm) [may be later]

----

# Convergence in Probability (or Weak Convergence)

**Definition (5.5.1):** A sequence of random variables $X_1,X_2,\dots$ **converges in probability** to a random variable $X$ if, for every $\varepsilon>0$, $$\lim_{n\to\infty}{P\left(|X_n-X|\geq\varepsilon\right)}=0,$$ or, equivalently, $$\lim_{n\to\infty}{P\left(|X_n-X|<\varepsilon\right)}=1.$$ To indicate this, we write $X_n\xrightarrow{P}X.$

\vspace{0.1in}

**Notes:**

1.  The random variables $X_1,X_2,\dots$ are NOT assumed to be $\operatorname{iid}$, i.e., we are in a more general setting than what we have discussed so far.

2.  We are often interested in the case where the "limiting" random variable $X$ is a constant (degenerate)

----

# Weak Law of Large Numbers (WLLN)

**Theorem 5.5.1.** Let $X_1,X_2,\dots$ be $\operatorname{iid}$ random variables with $\operatorname{E}(X_i)=\mu$ and $\operatorname{Var}(X_i)=\sigma^2<\infty$. Define $\overline{X}_n=\frac{1}{n}\sum_{i=1}^n{X_i}.$ Then for every $\varepsilon>0$, $$\lim_{n\to\infty}{P\left(|\overline{X}_n-\mu|<\varepsilon\right)}=1,$$ so that $$\overline{X}_n\xrightarrow{P}\mu.$$

**Proof:** Using Chebyshev. For any $\varepsilon> 0$ $$
P\left(|\overline{X}_n-\mu| \geq \varepsilon\right) \leq \frac{\operatorname{E}\left(\overline{X}_n-\mu\right)^2}{\varepsilon^2} = \frac{\operatorname{Var}(\overline X_n)}{\varepsilon^2} = \frac{\sigma^2}{n\varepsilon^2}.
$$

Hence $0 \leq P\left(|\overline{X}_n-\mu| \geq \varepsilon\right) \leq \frac{\sigma^2}{n\varepsilon^2} \to 0$ as $n \to \infty$.

----

**Remarks:**

1.  In other words, there is a very high probability that the sample mean can be made as close as we'd like to the population mean by taking $n$ sufficiently large.

2.  **Consistency:** when a sample quantity (statistic) converges in probability to a constant (more later).

\vspace{0.1in}

**Example:** Suppose we have a sequence $X_1,X_2,\dots,X_n$ are iid random variables with $\operatorname{E}(X_i) = \mu$ and $\operatorname{Var}(X_i) = \sigma^2$. Define $S_n^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \overline X_n)^2$. Can we prove a WLLN for $S_n^2?$

\vspace{0.1in}

Using Chebyshev $$
P\left(|S_n^2-\sigma^2| \geq \varepsilon\right) \leq \frac{\operatorname{E}\left(S_n^2-\sigma^2\right)^2}{\varepsilon^2} = \frac{\operatorname{Var}(S_n^2)}{\varepsilon^2}
$$ So a sufficient condition that $S_n^2 \xrightarrow{P} \sigma^2$ is that $\operatorname{Var}(S_n^2) \to 0$ as $n \to \infty$.

----

This sufficient condition holds if $X_1,X_2,\dots,X_n\sim \operatorname{iid~N}(\mu, \sigma^2)$. Then $(n-1)S_n^2/\sigma^2 \sim \chi^2_{n-1} \implies$ $$
\operatorname{Var}\left(\frac{(n-1)S_n^2}{\sigma^2}\right) = \frac{(n-1)^2}{\sigma^4} \operatorname{Var}(S_n^2) = \operatorname{Var}( \chi^2_{n-1} ) = 2(n-1)
$$ This implies $\operatorname{Var}(S_n^2) = \frac{2\sigma^4}{(n-1)} \to 0$ as $n \to \infty$.

\vspace{0.1in}

**Theorem 5.5.4.** Suppose that $X_1,X_2,\dots$ converges in probability to a random variable $X$, and that $h$ is a continuous function. Then $h(X_1),h(X_2),\ldots$ converges in probability to $h(X)$.

**Proof** Homework (see exercise $5.39$).

\vspace{0.1in}

**Example (contd.):** If $S_n^2 \to \sigma^2$ then $S_n = \sqrt{S_n^2} \to \sigma$.

----

# Almost Sure Convergence (or Strong Convergence)

**Definition (5.5.6):** A sequence of random variables $X_1,X_2,\ldots$ **converges almost surely** to a random variable $X$ if, for every $\varepsilon>0$, $$P\left(\lim_{n\to\infty}{|X_n-X|<\varepsilon}\right)=1.$$ To indicate this, we write $X_n\xrightarrow{a.s.}X.$

**Notes:**

-   Contrast this with the definition of convergence in probability.

-   Recall that a random variable is a function from a sample space $\mathcal{S}$ into the real numbers: $X_n:\mathcal{S}\longrightarrow\mathbb{R}$. For each $s\in S$, $X_n(s)=r\in\mathbb{R}$.

-   Definition 5.5.6 states that $X_n\xrightarrow{a.s.}X$ if the functions $X_n(s)\longrightarrow X(s)$ for all $s\in S$, except perhaps for $s\in N$, where $N \subseteq \mathcal{S}$ and $P(N)=0$ (point-wise convergence on all but a few "null" points).

<!-- --- -->

<!-- **Example:** Let the sample space $\mathcal{S}$ be the closed interval $[0,1]$ with the uniform probability distribution. Let $X_n(s)=s+s^n$ and $X(s)=s$.  Show that $X_n\xrightarrow{a.s.}X$. Does this sequence converge in probability? -->

<!-- \enter -->

<!-- For every $s \in [0, 1)$, $n \to \infty \implies s^n \to 0 \implies$   $X_n(s) \to s = X(s)$.  -->

<!-- For $s = 1$, $n \to \infty \implies s^n \to 1 \implies$   $X_n(s) \to 2 \neq 1 = X(s)$.  -->

<!-- But the convergence occurs on the set $[0, 1)$ and $P([0, 1)) = 1$. -->

<!-- So $X_n$ converges to $X$ almost surely. -->

<!-- --- -->

<!-- **Example:** Same $\mathcal{S} = [0,1]$ with the uniform probability distribution as before. Define the sequence $X_1,X_2,\dots$ as follows: -->

<!-- \begin{center} -->

<!-- \begin{tabular}{lll} -->

<!-- $X_1(s)=s+I_{[0,1]}(s)$ && $X_2(s)=s+I_{[0,1/2]}(s)$\\[1em] -->

<!-- $X_3(s)=s+I_{[1/2,1]}(s)$ && $X_4(s)=s+I_{[0,1/3]}(s)$\\[1em] -->

<!-- $X_5(s)=s+I_{[1/3,2/3]}(s)$ && $X_6(s)=s+I_{[2/3,1]}(s)$,\\ -->

<!-- \end{tabular} -->

<!-- \end{center} -->

<!-- and so on, and let $X(s)=s$. Show that this sequence converges in probability, but not almost surely. For any $\epsilon > 0$ -->

<!-- $P\left(|X_n-X|\geq\varepsilon\right) = P(\text{interval whose length is going to zero})$ $\to 0$. -->

<!-- For every $s$ the value $X_n(s)$ alternates between $s$ sand $s + 1$ infinitely often. For example, if $s = 3/8$, $X_1(s) = 11/8$ , $X_2(s) = 11/8$, $X_3(s) = 3/8$, $X_4(s) = 3/8$, $X_5(s) = 11/8$, $X_6(s) = 3/8$ etc. So no point-wise convergence occurs for this sequence. So $X_n$ does not converge almost surely. -->

<!-- --- -->

<!-- # Relationship between convergence in probability and convergence almost surely -->

<!-- - convergence almost surely _implies_ convergence in probability, but the converse is not true in general -->

<!-- \enter -->

<!-- - However, a sequence that converges in probability has a _sub-sequence_ that converges almost surely. -->

<!-- --- -->

<!-- # Strong Law of Large Numbers (SLLN) -->

<!-- Let $X_1,X_2,\ldots$ be $\on{iid}$ random variables with $\on{E}(X_i)=\mu$ and $\on{Var}(X_i)=\sigma^2<\infty$. Define -->

<!-- $\overline{X}_n=\frac{1}{n}\sum_{i=1}^n{X_i}.$ Then for every $\epsilon>0$, -->

<!-- $$P\left(\lim_{n\to\infty}{|\overline{X}_n-\mu|<\epsilon}\right)=1,$$ -->

<!-- so that -->

<!-- $$\overline{X}_n\xrightarrow{a.s.}\mu.$$ -->

<!-- **Remarks** -->

<!-- 1. "Stronger" analog of SLLN -->

<!-- 2. For both the WLLN and SLLN we had the assumption of a finite variance is sufficient but not necessary. The only moment condition needed is that -->

<!-- $\on{E}|X_i| < \infty$. -->

----

# Homework

-   Order Statistics: Read p. $226 - 232$. Exercises $5.22, 5.26$.

-   Convergence: Read p. $232 - 236$. Exercises $5.32, 5.38, 5.39a$.
