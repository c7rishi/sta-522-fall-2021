---
title: |
  | STA 522, Spring 2022  
  | Introduction to Theoretical Statistics II
author: | 
  | Lecture 11
  | 
  | Department of Biostatistics
  | University at Buffalo 
date: ""
output: 
  beamer_presentation:
    toc: false
header-includes:
  - \usepackage{bm}
  - \usepackage{amsmath}
  - \usepackage{mathrsfs}
bibliography: references.bib
fontsize: 10pt
editor_options: 
  markdown: 
    wrap: 80
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


\newcommand{\rs}{X_1,X_2,\dots,X_n}
\newcommand{\on}{\operatorname}
\newcommand{\enter}{\vspace{0.1in}}
\newcommand{\ds}{\displaystyle}
\renewcommand{\bar}{\overline}
\newcommand{\N}{\text{N}}
\renewcommand{\epsilon}{\varepsilon}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Ss}{\mathcal{S}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\E}{\on{E}}
\newcommand{\var}{\on{Var}}
\newcommand{\cov}{\on{Cov}}
\newcommand{\MSE}{\on{MSE}}
\renewcommand{\vec}{\underline}
\newcommand{\asim}{\stackrel{a}{\sim}}
\renewcommand{\mathbf}{\vec}
<!-- \renewcommand{\mathcal}{\mathscr} -->


## AGENDA

\vspace{0.1in}

- Extending Neyman-Pearson Lemma, MLR family

\vspace{0.1in}

- non-existence of UMP tests

\enter

- Interval Estimation

\enter

- Method of Finding Interval Estimates


---


# Review: Neyman Pearson Lemma & Most Powerful Tests

 - Consider testing $H_0: \theta = \theta_0$ vs. $H_1: \theta=\theta_1$,
where (1) the pdf or pmf corresponding to $\theta_i$ is $f(\mathbf{x}\,|\,\theta_i)$ for $i=0,1$; (2) the test has a rejection region $R$ that satisfies 
$\ds \mathbf{x}\in R\quad\text{if}\quad f(\mathbf{x}\,|\,\theta_1)>kf(\mathbf{x}\,|\,\theta_0)$
and
$\ds \mathbf{x}\in R^c\quad\text{if}\quad f(\mathbf{x}\,|\,\theta_1)<kf(\mathbf{x}\,|\,\theta_0)$
for some $k\geq0$; and (3) $\alpha=P_{\theta_0}(\mathbf{X}\in R)$.

- Then  (a) **(Sufficiency)** any test that satisfies (2) and (3) above is a UMP level $\alpha$ test; and (b) **(Necessity)** if there exists a test satisfying (2) and (3) above with $k>0$, then every UMP level $\alpha$ test is a size $\alpha$ test (satisfies (3) above), and every UMP level $\alpha$ test satisfies (2) above, except perhaps on a set $A$ satisfying
$\ds P_{\theta_0}(\mathbf{X}\in A)=P_{\theta_1}(\mathbf{X}\in A)=0.$


- Suppose $T(\mathbf{X})$ is a sufficient statistic for $\theta$, and let $g(t\,|\,\theta_i)$ be the pdf or pmf of $T$ corresponding to $\theta_i$ for $i=0,1$. Then any test based on $T$ with rejection region $S$ (a subset of the sample space of $T$) is a UMP level $\alpha$ test if it satisfies (1) for some $k\geq0$, $\ds t\in S \text{ if } g(t\,|\,\theta_1)>kg(t\,|\,\theta_0)$  and
$\ds t\in S^c \text{ if } g(t\,|\,\theta_1)<kg(t\,|\,\theta_0)$


---



# Extending the Neyman-Pearson Lemma


-  Can we extend the Neyman-Pearson Lemma to composite hypotheses (hypotheses that specify more than one possible distribution for the sample)?\vfill

  --  Yes, but only for one-sided hypotheses ($H:\theta\geq\theta_0$ or $H:\theta<\theta_0$). \vfill

  --  A UMP level $\alpha$ test must be UMP for all values in the alternative hypothesis.


---

# Monotone Likelihood Ratio (MLR)


**Definition:** A family of pdfs or pmfs $\lbrace g(t\,|\,\theta)\,:\,\theta\in\Theta\rbrace$ for a univariate random variable $T$ with real-valued parameter $\theta$ has a \textbf{monotone likelihood ratio (MLR)} if, for every $\theta_2>\theta_1$,
$$\frac{g(t\,|\,\theta_2)}{g(t\,|\,\theta_1)}$$
is a monotone (non-increasing or non-decreasing) function of $t$ on
$$\lbrace t\,:\,g(t\,|\,\theta_1)>0\text{~or~}g(t\,|\,\theta_2)>0\rbrace.$$

\enter \vfill

## Comments About MLR


- MLR is a property of a family of distributions.\vfill

- $\operatorname{N}(\theta,\sigma^2)$ (with $\sigma^2$ known), $\operatorname{poisson}(\theta)$, and $\operatorname{binomial}(n,\theta)$ all have an MLR.\vfill

- In general, any regular exponential family

---


# Karlin-Rubin Theorem


## Theorem

Consider testing $H_0: \theta\leq\theta_0$
vs. $H_1: \theta>\theta_0$. Suppose that $T$ is a sufficient statistic for $\theta$ and the family of pdfs or pmfs $\lbrace g(t\,|\,\theta)\,:\,\theta\in\Theta\rbrace$ of $T$ has an MLR. Then for any $t_0$, the test that rejects $H_0$ if and only if $T>t_0$ is a UMP level $\alpha$ test, where $\alpha=P_{\theta_0}(T>t_0)$.

\vspace{0.3in}

\enter

\enter

\enter

\vfill

**Example (Contd.):** Let $\rs \sim \on{iid} N(\theta, \sigma^2)$ population, $\sigma^2$ known. Consider testing $H_0' : \theta \geq \theta_0$ vs. $H_1' : \theta < \theta_0$.

\enter

Consider the test that rejects $H_0'$ if $\bar X < \theta_0 - z_\alpha \frac{\sigma}{\sqrt n}$. $\bar X$ is sufficient.

\enter

We'll show that the distribution of $T = \bar X$ has an MLR, and apply the Karlin-Rubin theorem. 

---

For $\theta_2 > \theta_1$: 
$$
\begin{aligned}
\frac{g(t \mid \theta_1)}{g(t \mid \theta_2)} &= \frac{\exp\left(-\frac{n}{2\sigma^2} (t - \theta_2)^2 \right)}{\exp\left(-\frac{n}{2\sigma^2} (t - \theta_1)^2 \right)} \\
&= \exp\left[ \frac{n}{\sigma^2} t (\theta_2 - \theta_1) \right] \exp\left[ -\frac{n}{2\sigma^2} (\theta_2^2 - \theta_1^2) \right]
\end{aligned}
$$
which is non-decreasing in $t$ as $\theta_2 - \theta_1 > 0$. 

\enter

Thus the distribution of $T = \bar X$ has an MLR.

\enter

\enter

Therefore, from Karlin-Rubin theorem it follows that this test is UMP level $\alpha$ for this problem.


---

#  Nonexistence of UMP Test

**Example:** Let $\rs\sim\operatorname{iid~N}(\theta,\sigma^2)$, with $\sigma^2$ known. 
Consider testing
$$
\begin{aligned}
H_0 & : \theta=\theta_0\\
\text{vs. } H_1 & : \theta\neq\theta_0.
\end{aligned}
$$
We'll show that there does not any UMP test at any level $0 < \alpha < 1$. 

\enter



For a specified value of $\alpha$, a level $\alpha$ test in this problem is any test that satisfies
$$P_{\theta_0}(\text{reject~}H_0)\leq\alpha.$$

\enter


Suppose $\theta_1<\theta_0$. 
By Corollary to the NP Lemma with sufficient statistic, the test with rejection region
$$R=\Big\lbrace\mathbf{x}\,:\,\overline{x}<\theta_0-\frac{\sigma z_{\alpha}}{\sqrt{n}}\Big\rbrace$$
has the highest possible power at $\theta_1$; call this Test 1.


---

By part (b) of the NP Lemma, any other level $\alpha$ test that has the same power as Test 1 at $\theta_1$ must have the same rejection region, except possibly for a set $A$ with measure zero.

\enter

So if a UMP level $\alpha$ test exists, it must be Test 1, since no other level $\alpha$ test has as high a power as Test 1 at $\theta_1$.


\enter

Now consider Test 2, which has rejection region
$$R=\Big\lbrace\mathbf{x}\,:\,\overline{x}>\theta_0+\frac{\sigma z_{\alpha}}{\sqrt{n}}\Big\rbrace.$$
This is also a level $\alpha$ test.

\enter


We can show that for any $\theta_2>\theta_0$, $\beta_2(\theta_2)>\beta_1(\theta_2)$.

\enter

So Test 1 cannot be a UMP level $\alpha$ test, since Test 2 has a higher power than Test 1 at $\theta_2$.

\enter

Therefore, no UMP level $\alpha$ test exists in this problem.

---

Since a global UMP test does not exist, we can restrict to the class of unbiased tests. (Recall that for an unbiased test the power function at each $\theta \in \Theta_0^c$ is $\geq$ the level of the test.)\vfill

\enter

Consider Test 3, which rejects $H_0: \theta = \theta_0$ in favor of $H_1: \theta \neq \theta_0$ if and only if 
$$
\bar X > \theta_0 + \sigma z_{\alpha/2}/\sqrt{n} \ \text{ or } \ \bar X < \theta_0 - \sigma z_{\alpha/2}/\sqrt{n}
$$
is actually a UMP unbiased level $\alpha$ test; i.e., it is UMP in the class of unbiased tests.

---

# $p$-Values


**Defintion:** A \textbf{$p$-value}, $p(\mathbf{X})$, is a test statistic satisfying $0\leq p(\mathbf{x})\leq1$ for every sample point $\mathbf{x}$. Small values of $p(\mathbf{X})$ give evidence that $H_1$ is true. A $p$-value is \textbf{valid} if, for every $\theta\in\Theta_0$ and every $0\leq\alpha\leq1$, 
$$P_{\theta}(p(\mathbf{X})\leq\alpha)\leq\alpha.$$

\enter

If $p(\mathbf{X})$ is a valid $p$-value, then the test that rejects $H_0$ if and only if $p(\mathbf{X})\leq\alpha$ is a level $\alpha$ test.


\enter 
\enter \vfill

## Theorem (8.3.27; Determining Valid $p$-Values)
Let $W(\mathbf{X})$ be a test statistic such that large values of $W$ give evidence that $H_1$ is true. For each sample point $\mathbf{x}$, define
$$p(\mathbf{x})=\operatorname{sup}_{\theta\in\Theta_0}{P_{\theta}\left[W(\mathbf{X})\geq W(\mathbf{x})\right]}.$$
Then $p(\mathbf{X})$ is a valid $p$-value.


<!-- --- -->

<!-- **Proof:** Fix $\theta\in\Theta_0$. Let $F_\theta(w)$ denote the cdf of $-W(X)$. Define -->
<!-- $$p_\theta(\vec x) = P_{\theta}\left[W(\mathbf{X})\geq W(\mathbf{x})\right] = P_{\theta}\left[-W(\mathbf{X})\leq -W(\mathbf{x})\right] = F_\theta(-W(x)).$$ -->
<!-- Then the random variable $p_\theta(\vec X)$ is equal to $F_\theta(-W(\vec X))$.  -->

<!-- \enter -->

<!-- Hence, by the Probability Integral Transformation $P_\theta(p_\theta(\mathbf X) \leq \alpha)$. Since  -->
<!-- $$ -->
<!-- p(\vec x) = \operatorname{sup}_{\theta'\in\Theta_0} p_{\theta'}(\vec x) \geq p_\theta(\vec x) -->
<!-- $$  -->
<!-- for all $\vec x$, we have -->
<!-- $$ -->
<!-- P_\theta(p(\vec X) \leq \alpha \ ) \leq P_\theta(p_\theta(\vec X) \leq \alpha) \leq \alpha -->
<!-- $$ -->
<!-- which is true for all $\theta \in \Theta_0$ and for every $0 \leq \alpha \leq 1$. -->

<!-- Hence $p(\vec X)$ is a valid $p$-value. -->

---


# Interval Estimation

**Defintion:** An \textbf{interval estimate} of a real-valued parameter $\theta$ is any pair of functions, $L(\mathbf{x})$ and $U(\mathbf{x})$, of a sample that satisfy $L(\mathbf{x})\leq U(\mathbf{x})$ for all $\mathbf{x}\in\mathcal{X}$. If $\mathbf{X}=\mathbf{x}$ is observed, the inference $L(\mathbf{x})\leq\theta\leq U(\mathbf{x})$ is made. The random interval $[L(\mathbf{X}),U(\mathbf{X})]$ is called an \textbf{interval estimator}.

\vspace{0.2in}

**Example:** Suppose $X_1,X_2,X_3,X_4\sim\operatorname{iid~N}(\mu,1)$. Then $[\overline{X}-1,\overline{X}+1]$ is an interval estimator for the population mean $\mu$. What is $P\left(\mu\in[\overline{X}-1,\overline{X}+1]\right)$?

\vspace{0.2in}

Note that interval estimators are less precise than point estimators, but are more likely to be correct.
\vfill
Recall that $P(\overline{X}=\mu)=0$, for instance, i.e., there is no chance we are correct if we estimate $\mu$ using $\overline{X}$.


---

# Coverage Probability & Confidence coefficient

**Definition:** For an interval estimator $[L(\mathbf{X}),U(\mathbf{X})]$ of a parameter $\theta$, the \textbf{coverage probability} of $[L(\mathbf{X}),U(\mathbf{X})]$ is the probability that the random interval $[L(\mathbf{X}),U(\mathbf{X})]$ covers the true parameter $\theta$. 

Symbolically, it is denoted by either $P_{\theta}\left(\theta\in [L(\mathbf{X}),U(\mathbf{X})]\right)$ or $P\left(\theta\in [L(\mathbf{X}),U(\mathbf{X})]\,|\,\theta\right)$. 

Note that the coverage probability is usually a function of $\theta$.

\vskip 0.2in


**Definition:** For an interval estimator $[L(\mathbf{X}),U(\mathbf{X})]$ of a parameter $\theta$, the \textbf{confidence coefficient} of $[L(\mathbf{X}),U(\mathbf{X})]$ is the infimum of the coverage probabilities,
$$\inf_{\theta}{P_{\theta}\left(\theta\in [L(\mathbf{X}),U(\mathbf{X})]\right)}.$$
We use \textbf{confidence interval} to mean the interval estimator along with its corresponding confidence coefficient.

\enter

Note that since $\theta$ is fixed, but unknown, the probability statements above refer to $\mathbf{X}$, not $\theta$. We can think of such a probability as $P_{\theta}\left(L(\mathbf{X})\leq\theta,U(\mathbf{X})\geq\theta\right)$.


---

**Example (Scale Uniform Interval Estimator):**
Let $\rs$ be a random sample from a $\operatorname{uniform}(0,\theta)$ population, and let $Y=X_{(n)}$ be the $n$th order statistic.
\vskip .2in
We are interested in an interval estimator of $\theta$.
\vskip .2in
We consider two candidate estimators:
\vskip .1in
\begin{enumerate}[(a)]
\item $[aY,bY]$, where $1\leq a<b$; and\vskip .1in
\item $[Y+c,Y+d]$, where $0\leq c<d$.
\end{enumerate}
\vskip .2in
Note that $\theta$ is necessarily larger than $y$.
\vskip .2in
Determine the coverage probability and confidence coefficient for each estimator. 

---

(a) We have
$$
\begin{aligned}
P_\theta (\theta \in [aY, bY]) &= P_\theta(aY \leq \theta \leq bY) \\
&= P_\theta \left( \frac{1}{b} \leq \frac{Y}{\theta} \leq \frac{1}{a} \right) \\
&= P_\theta \left( \frac{1}{b} \leq T \leq \frac{1}{a} \right) && \left(T = \frac{Y}{\theta}\right)
\end{aligned}
$$
The pdf of $T$ is $f_T(t) = nt^{n-1}$, $0 \leq t \leq 1$. Therefore,
$$
P_\theta (\theta \in [aY, bY]) =  P_\theta \left( \frac{1}{b} \leq T \leq \frac{1}{a} \right) = \int_{1/b}^{1/a} nt^{n-1} \ dt = \left(\frac{1}{a}\right)^n - \left(\frac{1}{b}\right)^n
$$
which is free of $\theta$. So, confidence coefficient = 
$$
\inf_\theta P_\theta (\theta \in [aY, bY]) = \left(\frac{1}{a}\right)^n - \left(\frac{1}{b}\right)^n 
$$

---

(b) Here
$$
P_\theta(\theta \in [Y+c, Y+ d]) = \left(1 - \frac{c}{\theta} \right)^n - \left(1 - \frac{d}{\theta} \right)^n
$$
which depends on $\theta$. Note that
$$
\lim_{\theta \to \infty} P_\theta(\theta \in [Y+c, Y+ d]) = \lim_{\theta \to \infty} \left[ \left(1 - \frac{c}{\theta} \right)^n - \left(1 - \frac{d}{\theta} \right)^n \right] = 0
$$

So, confidence coefficient = $\inf_\theta P_\theta (\theta \in[Y+c, Y+ d]) = 0$.

---

# Methods of Finding Interval Estimators

(a) Invert a test statistic.\vfill

(b) Use pivotal quantities. \vfill


## Correspondence Between Confidence Intervals and Hypothesis Testing

- There is a very strong correspondence between hypothesis testing and interval estimation.\vfill

- In general, every confidence interval corresponds to a test, and vice versa.

---

**Example (Inverting a Normal Test):** Let $\rs\sim\operatorname{iid~N}(\mu,\sigma^2)$.
Suppose we are testing $H_0  : \mu=\mu_0$
vs. $H_1  : \mu\neq\mu_0$. Consider the rejection region
$$R=\left\lbrace\mathbf{x}\,:\,|\overline{x}-\mu_0|>z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\right\rbrace.$$
The acceptance region of the hypothesis test (the subset of the sample space for which $H_0:\mu=\mu_0$ is accepted) is
$$
\begin{aligned}
A(\mu_0) & = \left\lbrace\mathbf{x}\,:\,\mu_0-z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\leq\overline{x}\leq\mu_0+z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\right\rbrace \\
& = \left\lbrace\mathbf{x}\,:\,\overline{x}-z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\leq\mu_0\leq\overline{x}+z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\right\rbrace.
\end{aligned}
$$


Note that since $P(\mathbf{x}\in R\,|\,\mu=\mu_0)=\alpha$, we can deduce that
$$P(\mathbf{x}\in A(\mu_0)\,|\,\mu=\mu_0)=1-\alpha$$
for every $\mu_0$.

---

So $P_{\mu}(\mathbf{x}\in A(\mu))=1-\alpha$.

\enter

The $1-\alpha$ confidence interval (the subset of the parameter space containing plausible values of $\mu$) is
$$C(\mathbf{x})=\left\lbrace\mu\,:\,\overline{x}-z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\leq\mu\leq\overline{x}+z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\right\rbrace.$$
\vskip .2in
So we see that
$$\mathbf{x}\in A(\mu_0)\iff\mu_0\in C(\mathbf{x}),$$
i.e., $\mathbf{x}$ is in the acceptance region for $H_0:\mu=\mu_0$ if and only if $\mu_0$ is a plausible value for the parameter $\mu$.

---

# Correspondence Between Confidence Intervals and Hypothesis Testing


- Both hypothesis tests and confidence intervals look for consistency between sample statistics and population parameters.\vfill


- The hypothesis test fixes the parameter and asks what sample values are consistent with that fixed value (the acceptance region).\vfill


- The confidence interval fixes the sample value and asks what parameter values make this sample value most plausible (the confidence interval).


<!-- --- -->

<!-- # Homework -->

<!-- -  Hypothesis tests: Read p. $388-399$. -->

<!-- -  Interval Estimation: Read p. $417-421$. -->

<!-- -   Exercises: TBA. -->

